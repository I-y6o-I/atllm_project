[Page 3]
LSTM architectures, we also evaluated AWD-
LSTM (Merity et al., 2018b) on PTB. For the
non-neural approach, we used a standard ﬁve-
gram model with modiﬁed Kneser-Ney smooth-
ing (Chen and Goodman, 1996), as explored in
Mikolov and Zweig (2012) on PTB. We de-
note the QRNN models for PTB and WT103 as
ptb-qrnn andwt103-qrnn , respectively.
For each model, we examined word-level per-
plexity, R@3 in next-word prediction, latency
(ms/q), and energy usage (mJ/q). To explore the